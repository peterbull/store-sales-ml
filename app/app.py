# AUTOGENERATED! DO NOT EDIT! File to edit: ../store_sales_2.ipynb.

# %% auto 0
__all__ = ['iskaggle', 'creds', 'cred_path', 'path', 'train_df', 'test_df', 'sub_df', 'stores_df', 'oil_df', 'hol_events_df',
           'transactions_df', 'combined_df', 'test_idxs', 'train_idxs', 'valid_idxs', 'eq_start_date', 'eq_end_date',
           'earthquake_cond', 'earthquake_indexes', 'dep_var', 'procs', 'cont', 'cat', 'train_val_splits', 'to',
           'test_to', 'xs', 'y', 'valid_xs', 'valid_y', 'test_xs', 'm', 'dls', 'learn', 'preds', 'targs', 'nn_r_mse',
           'rf_preds', 'ens_preds', 'rf_test_preds', 'dl', 'np_preds', 'ens_preds_norm', 'r_mse', 'm_rmse', 'rf']

# %% ../store_sales_2.ipynb 1
from fastai.tabular.all import *
from fastbook import *

from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_log_error

import xgboost as xgb

import calendar

import seaborn as sns

from dtreeviz.trees import *
import dtreeviz

from treeinterpreter import treeinterpreter as ti
import waterfall_chart

from fastprogress import master_bar, progress_bar
from fastprogress.fastprogress import force_console_behavior


# %% ../store_sales_2.ipynb 3
iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')
creds = ''

# %% ../store_sales_2.ipynb 4
cred_path = Path('~/.kaggle/kaggle.json').expanduser()
if not cred_path.exists():
    cred_path.parent.mkdir(exist_ok=True)
    cred_path.write_text(creds)
    cred_path.chmod(0o600)

# %% ../store_sales_2.ipynb 5
path = Path('store-sales-time-series-forecasting')

# %% ../store_sales_2.ipynb 6
if not iskaggle and not path.exists():
    import zipfile, kaggle
    kaggle.api.competition_download_cli(str(path))    
    zipfile.ZipFile(f'{path}.zip').extractall(path)


# %% ../store_sales_2.ipynb 7
if iskaggle:
    path = Path('../input/store-sales-time-series-forecasting')
    ! pip install -q dataset

# %% ../store_sales_2.ipynb 9
train_df = pd.read_csv(path/'train.csv', low_memory=False)
test_df = pd.read_csv(path/'test.csv', low_memory=False)
sub_df = pd.read_csv(path/'sample_submission.csv', low_memory=False)
stores_df = pd.read_csv(path/'stores.csv', low_memory=False)
oil_df = pd.read_csv(path/'oil.csv', low_memory=False)
hol_events_df = pd.read_csv(path/'holidays_events.csv', low_memory=False)
transactions_df = pd.read_csv(path/'transactions.csv', low_memory=False)

# %% ../store_sales_2.ipynb 12
combined_df = pd.concat([train_df, test_df]).reset_index()

# %% ../store_sales_2.ipynb 14
test_idxs = combined_df.index[(combined_df.index > train_df.index.max())] 

# %% ../store_sales_2.ipynb 16
train_idxs = combined_df.index[(combined_df.index < round(len(train_df) * 0.8))]

# %% ../store_sales_2.ipynb 17
valid_idxs = combined_df.index[(combined_df.index > len(train_idxs)) & (combined_df.index < test_idxs.min())]

# %% ../store_sales_2.ipynb 19
combined_df = combined_df.merge(oil_df, on='date', how='left')

# %% ../store_sales_2.ipynb 21
combined_df = combined_df.merge(stores_df, on='store_nbr', how='left')

# %% ../store_sales_2.ipynb 24
hol_events_df.rename(columns={'type': 'hol_type'}, inplace=True)

# %% ../store_sales_2.ipynb 25
hol_events_df.drop_duplicates(subset='date', keep='first', inplace=True)

# %% ../store_sales_2.ipynb 26
combined_df = combined_df.merge(hol_events_df, on='date', how='left')

# %% ../store_sales_2.ipynb 30
# combined_df = combined_df.merge(transactions_df, on=['store_nbr', 'date'], how='left')

# %% ../store_sales_2.ipynb 31
# combined_df['transactions'] = combined_df.groupby(['date', 'store_nbr'])['transactions'].transform(lambda x: x.fillna(method='ffill'))

# %% ../store_sales_2.ipynb 33
combined_df['date'] = pd.to_datetime(combined_df['date'])

# %% ../store_sales_2.ipynb 38
eq_start_date = pd.to_datetime("2016-04-16")
eq_end_date = pd.to_datetime("2016-05-16")

# %% ../store_sales_2.ipynb 39
earthquake_cond = (combined_df.date >= eq_start_date) & (combined_df.date < eq_end_date)

# %% ../store_sales_2.ipynb 41
earthquake_indexes = combined_df.index[earthquake_cond]

# %% ../store_sales_2.ipynb 43
combined_df = add_datepart(combined_df, 'date')

# %% ../store_sales_2.ipynb 45
# combined_df.drop(earthquake_indexes, inplace=True)

# %% ../store_sales_2.ipynb 47
dep_var = 'sales'

# %% ../store_sales_2.ipynb 49
combined_df[dep_var] = np.log(combined_df[dep_var] + 1e-5)

# %% ../store_sales_2.ipynb 51
procs = [Categorify, FillMissing, Normalize]

# %% ../store_sales_2.ipynb 53
cont, cat = cont_cat_split(combined_df, 1, dep_var=dep_var)

# %% ../store_sales_2.ipynb 55
train_val_splits = (list(train_idxs), list(valid_idxs))

# %% ../store_sales_2.ipynb 57
to = TabularPandas(combined_df, procs, cat, cont, y_names=dep_var, splits=train_val_splits)

# %% ../store_sales_2.ipynb 58
test_to = TabularPandas(combined_df.iloc[test_idxs], procs, cat, cont, y_names=None, splits=None)

# %% ../store_sales_2.ipynb 60
xs, y = to.train.xs, to.train.y
valid_xs, valid_y = to.valid.xs, to.valid.y

# %% ../store_sales_2.ipynb 61
test_xs = test_to.train.xs

# %% ../store_sales_2.ipynb 63
def r_mse(pred, y):
    return round(math.sqrt(((pred-y)**2).mean()), 6)

# %% ../store_sales_2.ipynb 64
def m_rmse(m, xs, y):
    return r_mse(m.predict(xs), y)

# %% ../store_sales_2.ipynb 66
def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs):
    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, 
                                 max_samples=max_samples, max_features=max_features,
                                 min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)

# %% ../store_sales_2.ipynb 68
m = rf(xs, y)

# %% ../store_sales_2.ipynb 91
dls = to.dataloaders(1024)

# %% ../store_sales_2.ipynb 94
learn = tabular_learner(dls, layers=[500, 250], n_out=1, y_range=(-11,11), loss_func=F.mse_loss)

# %% ../store_sales_2.ipynb 96
learn.fit_one_cycle(5, 1e-2)

# %% ../store_sales_2.ipynb 98
preds, targs = learn.get_preds()
nn_r_mse = r_mse(preds, targs)
nn_r_mse

# %% ../store_sales_2.ipynb 99
rf_preds = m.predict(valid_xs)

# %% ../store_sales_2.ipynb 100
ens_preds = (to_np(preds.squeeze()) + rf_preds) /2

# %% ../store_sales_2.ipynb 102
rf_test_preds = m.predict(test_xs)

# %% ../store_sales_2.ipynb 103
dl = learn.dls.test_dl(combined_df.iloc[test_idxs])

# %% ../store_sales_2.ipynb 104
preds = learn.get_preds(dl=dl)

# %% ../store_sales_2.ipynb 107
np_preds = to_np(preds[0]).reshape(-1)

# %% ../store_sales_2.ipynb 109
ens_preds = (np_preds + rf_test_preds) / 2

# %% ../store_sales_2.ipynb 111
learn.save('nn2')

# %% ../store_sales_2.ipynb 113
ens_preds_norm = np.exp(ens_preds + 1e-5)

# %% ../store_sales_2.ipynb 115
sub_df['sales'] = ens_preds_norm

# %% ../store_sales_2.ipynb 118
sub_df.to_csv('submission.csv', index=False)
